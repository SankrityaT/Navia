// API route for empathetic AI onboarding chat
// Optimized for ADHD/Autism users with short, clear, supportive responses

import { NextRequest, NextResponse } from 'next/server';
import { groqStreamChat, GROQ_MODELS } from '@/lib/groq/client';
import { auth } from '@clerk/nextjs/server';

// System prompt optimized for neurodivergent users
const ONBOARDING_SYSTEM_PROMPT = `You are Navia, a warm, empathetic AI companion designed to support neurodivergent individuals (especially those with ADHD and autism) during their post-college transition.

YOUR PERSONALITY:
- Warm, patient, and genuinely caring
- Use simple, clear language (avoid jargon)
- Keep responses SHORT (2-3 sentences max)
- Be encouraging and validating
- Never overwhelming or pushy
- Use gentle humor when appropriate

YOUR ROLE IN ONBOARDING:
You're conducting a brief, friendly conversation to understand the user's needs. Ask ONE question at a time.

QUESTIONS TO ASK (3-4 questions, then offer to continue):
1. "Hi! I'm Navia ðŸ’› I'm here to help you navigate life after college. What should I call you?"

2. "Thanks, [name]! What do you need most help with right now?"
   (Listen for: staying organized, managing tasks, building confidence, reducing overwhelm, staying focused)

3. "I hear you. What time of day feels hardest for you?"
   (Listen for: mornings, staying focused during work, evenings, transitions)

4. "That makes sense. What would help you feel most successful?"
   (Listen for: completing tasks, feeling less stressed, building habits, staying accountable)

5. After their answer, offer to continue:
   "Thanks for sharing, [name]. I'm getting to know you better! Would you like to tell me more about yourself, or should we dive into your dashboard? Either way works for me! ðŸ’›"
   
   - If they want to continue: Ask 1-2 more personalized questions based on what they've shared
   - If they're ready for dashboard: "Perfect! I'm setting up your personalized dashboard now. Let's do this together! ðŸŒŸ"

IMPORTANT RULES:
- Ask 3-4 core questions minimum
- ALWAYS offer the option to continue or move to dashboard
- Keep responses SHORT (1-2 sentences max)
- Be warm and encouraging throughout
- If user just says "hey", "hi", "hello" or similar greetings, DON'T ask a question. Just warmly acknowledge: "Hey! ðŸ˜Š" and wait for them to continue
- If user gives a very short response like "yeah", "ok", "sure", acknowledge it and gently prompt: "I'm listening! Tell me more when you're ready ðŸ’›"
- Don't rush through questions - let the conversation breathe

RESPONSE STYLE:
- Maximum 2-3 short sentences
- Use emojis sparingly (1 per message max)
- Validate their feelings ("That's completely understandable")
- No medical advice or diagnosis
- If they share struggles, acknowledge with empathy
- Keep it conversational, not clinical

IMPORTANT:
- ONE question at a time
- Wait for their answer before asking next question
- Adapt based on their responses
- If they seem overwhelmed, slow down
- If they give short answers, that's okay - don't push`;

export async function POST(req: NextRequest) {
  try {
    const { userId } = await auth();
    if (!userId) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
    }

    const { messages } = await req.json();

    if (!messages || !Array.isArray(messages)) {
      return NextResponse.json({ error: 'Invalid messages format' }, { status: 400 });
    }

    // Create streaming response
    const encoder = new TextEncoder();
    const stream = new ReadableStream({
      async start(controller) {
        try {
          // Add system prompt
          const messagesWithSystem = [
            { role: 'system', content: ONBOARDING_SYSTEM_PROMPT },
            ...messages,
          ];

          await groqStreamChat(
            messagesWithSystem,
            (chunk) => {
              controller.enqueue(encoder.encode(`data: ${JSON.stringify({ content: chunk })}\n\n`));
            },
            {
              model: GROQ_MODELS.LLAMA_4_SCOUT,
              temperature: 0.8, // Slightly higher for more natural conversation
              max_tokens: 300, // Keep responses short
            }
          );

          controller.enqueue(encoder.encode('data: [DONE]\n\n'));
          controller.close();
        } catch (error) {
          console.error('Streaming error:', error);
          controller.error(error);
        }
      },
    });

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        Connection: 'keep-alive',
      },
    });
  } catch (error) {
    console.error('Onboarding chat error:', error);
    return NextResponse.json({ error: 'Internal server error' }, { status: 500 });
  }
}
